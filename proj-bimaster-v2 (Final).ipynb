{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11141182,"sourceType":"datasetVersion","datasetId":6949716},{"sourceId":11219039,"sourceType":"datasetVersion","datasetId":7006255}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PoC de RAG on-premises para pesquisa de artigos técnicos da área de Tecnologia da Informação\n<br>\n\n**Introdução**<br>\nEste projeto implementa um sistema de Geração Aumentada por Recuperação (RAG) para consultas a artigos técnicos na área de TI. O sistema foi desenvolvido para funcionar completamente *on-premises*, sem necessidade de envio de dados para serviços em nuvem, garantindo a privacidade e segurança de informações sensíveis. Por esta razão, modelos relativamente pequenos foram escolhidos para minimizar o consumo de recursos de processamento e de memória, mas também permite a substituição por modelos maiores ou mais especializados conforme a demanda e disponibilidade de recursos.\n\nOutro foco foi o uso de tecnologias e modelos disponíveis gratuitamente a fim de reduzir o impacto financeiro para a implementação da solução, sem gastos com assinatura de soluções comerciais com custo flutuante.\n\nPara fins de Prova de Conceito, precisávamos de uma base de dados de conhecimento disponível publicamente. A base utilizada foi a do Microsoft Learn sobre um recurso de configuração automatizada de dispositivos chamado de [Windows Autopilot](https://learn.microsoft.com/en-us/autopilot/). O recurso é utilizado em grandes corporações para personalizar laptops e desktops rodando sistema operacional Windows com as credenciais do usuário, aplicativos obrigatórios e as restrições da empresa no primeiro uso.\n\nA fonte foi escolhida por permitir fácil exportação de todo o material para PDF, por ser um recurso amplamente usado em grandes corporações e pela estrutura do documento ser primariamente textual, sem imagens.\n\n<img src=\"https://raw.githubusercontent.com/fabiofaria-git/BIMaster-Proj/refs/heads/main/MSLearn-Autopilot.png\" width=30% height=30% />\n\nO PDF resultante foi extraído em 19 de março de 2025 e possui 618 páginas. Como o conteúdo no Microsoft Learn é dinâmico e atualizado regularmente, extrações em datas posteriores podem ter resultados diferentes. Em seguida, foi feito o upload como um dataset do projeto na plataforma de escolha.\n\nA aplicação utiliza embeddings de texto com janelas de 1024 tokens e o índice vetorial FAISS para recuperar informações relevantes de documentos PDF, que são então utilizadas por um modelo de linguagem (LLM) para gerar respostas precisas às consultas dos usuários.\n<br><br>","metadata":{}},{"cell_type":"markdown","source":"# Instalação das dependências\n\nDevido à demanda de memória, este projeto foi totalmente desenvolvido na plataforma [Kaggle](https://www.kaggle.com/) que, até a data de publicação deste trabalho, oferecia um ambiente Jupyter Notebook com acesso gratuito a 2 GPUs com 15 GB de VRAM cada. Assim sendo, as bibliotecas instaladas nesta etapa são as que ainda não existiam nativamente na plataforma.\n\nAo utilizar outras plataformas como Google Colab pode ser necessário instalar outras bibliotecas.\n\n- faiss-gpu: para indexação e busca vetorial com aceleração por GPU\n- pdfplumber: para extração de texto de documentos PDF\n- langchain-community e langchain-huggingface: para integração com modelos transformers de linguagem\n- huggingface_hub: para acesso aos modelos do Hugging Face\n- huggingface_hub[hf_xet]: acelera download dos modelos\n\nA flag --quiet reduz a quantidade de saída durante a instalação.","metadata":{}},{"cell_type":"code","source":"!pip install -U faiss-gpu pdfplumber langchain-community langchain-huggingface huggingface_hub[cli] huggingface_hub[hf_xet] --quiet","metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2025-06-06T01:46:37.217210Z","iopub.execute_input":"2025-06-06T01:46:37.217539Z","iopub.status.idle":"2025-06-06T01:46:55.770127Z","shell.execute_reply.started":"2025-06-06T01:46:37.217512Z","shell.execute_reply":"2025-06-06T01:46:55.769177Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Importação de bibliotecas","metadata":{}},{"cell_type":"code","source":"import os\nimport faiss\nimport numpy as np\nimport logging\nimport json\nimport torch\nimport pdfplumber\nimport transformers\nimport re\nfrom tqdm import tqdm\nfrom typing import List, Dict, Tuple, Any\nfrom sklearn.preprocessing import normalize\nfrom langchain_huggingface import HuggingFacePipeline\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom huggingface_hub import login","metadata":{"execution":{"iopub.status.busy":"2025-06-06T01:46:55.771254Z","iopub.execute_input":"2025-06-06T01:46:55.771476Z","iopub.status.idle":"2025-06-06T01:47:02.635428Z","shell.execute_reply.started":"2025-06-06T01:46:55.771453Z","shell.execute_reply":"2025-06-06T01:47:02.634808Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Autenticação no Hugging Face\n\nPara que a chave de API não fique exposta no código, gere uma chave secreta no Hugging Face e adicione-a em uma variável secreta nos Segredos do Kaggle ou do Colab com o nome de \"HF_TOKEN\".\n\nComente/Descomente o código abaixo de acordo com a plataforma escolhida.","metadata":{}},{"cell_type":"code","source":"# Obter chave secreta de API no Kaggle.\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nlogin(token = user_secrets.get_secret(\"HF_TOKEN\"))\n\n# Obter chave secreta de API no Google Colab.\n# from google.colab import userdata\n# hf_token = userdata()\n# login(token = userdata.get('HF_TOKEN'))\n","metadata":{"execution":{"iopub.status.busy":"2025-06-06T01:47:02.636877Z","iopub.execute_input":"2025-06-06T01:47:02.637189Z","iopub.status.idle":"2025-06-06T01:47:02.905875Z","shell.execute_reply.started":"2025-06-06T01:47:02.637168Z","shell.execute_reply":"2025-06-06T01:47:02.905198Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Registro de Eventos (Logging)\n\nLogging foi usado em algumas etapas para registrar informações sobre o processo de extração, indexação e busca. Ajuda a diagnosticar problemas quando ocorrerem, além de prover informações sobre o progresso.","metadata":{}},{"cell_type":"code","source":"# Configurando o registro de eventos (logging).\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)","metadata":{"execution":{"iopub.status.busy":"2025-06-06T01:47:02.907401Z","iopub.execute_input":"2025-06-06T01:47:02.907763Z","iopub.status.idle":"2025-06-06T01:47:02.911435Z","shell.execute_reply.started":"2025-06-06T01:47:02.907729Z","shell.execute_reply":"2025-06-06T01:47:02.910505Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inicialização de variáveis\n\nDefine caminhos e nomes de arquivos para o sistema\n* **pdf_folder:** diretório contendo os documentos PDF\n* **index_file:** nome do arquivo para o índice FAISS\n* **metadata_file:** nome do arquivo para os metadados dos documentos\n* **embeddings_model_name:** Nome do modelo para geração de embeddings\n* **llm_model_id:** Nome do modelo de LLM para geração das respostas","metadata":{}},{"cell_type":"code","source":"pdf_folder = \"/kaggle/input/autopilotfullmanual\"\nindex_file = \"faiss_index.faiss\"\nmetadata_file = \"faiss_metadata.json\"\n\nembeddings_model_name = \"BAAI/bge-m3\"\nllm_model_id = \"meta-llama/Llama-3.2-3B-Instruct\"","metadata":{"execution":{"iopub.status.busy":"2025-06-06T01:47:02.912395Z","iopub.execute_input":"2025-06-06T01:47:02.912673Z","iopub.status.idle":"2025-06-06T01:47:02.928099Z","shell.execute_reply.started":"2025-06-06T01:47:02.912652Z","shell.execute_reply":"2025-06-06T01:47:02.927512Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Os modelos escolhidos para esta prova de conceito se destacaram pelos seguintes motivos:\n\n**BAAI/bge-m3** (para embeddings)\n\n* **Alto desempenho semântico:** Captura relações semânticas sofisticadas entre termos técnicos;\n* **Multilíngue:** Suporta múltiplos idiomas, incluindo português;\n* **Otimizado para retrieval:** Desenvolvido especificamente para sistemas de recuperação de informação\n* **Eficiência computacional:** Gera embeddings de alta qualidade com requisitos moderados de recursos;\n* **Excelente em consultas técnicas:** Particularmente forte em capturar a semântica de documentação de TI;\n* **Suporte a contextos longos:** Processa documentos mais extensos (até 8.192 tokens) sem perda significativa de qualidade;\n* **Dimensionalidade adequada:** Balanço ideal entre riqueza de representação e eficiência de armazenamento;\n* **Boa integração com FAISS**.\n\n**meta-llama/Llama-3.2-3B-Instruct** (para geração)\n\n* **Tamanho compacto:** Com 3B de parâmetros, oferece bom equilíbrio entre qualidade e requisitos de hardware;\n* **Fine-tuned para Instruções:** Otimizado para seguir instruções e responder perguntas com base em contexto;\n* **Forte em resumos técnicos:** Capacidade de sintetizar informações de múltiplos fragmentos técnicos;\n* **Multilíngue:** Suporta ainda mais idiomas que o **BAAI/bge-m3**, incluindo o português;\n* **Desempenho competitivo:** Qualidade de resposta comparável a modelos muito maiores;\n* **Baixo uso de VRAM**;\n* **Inferência rápida**;\n* **Formato de resposta estruturado:** Gera respostas bem organizadas, úteis em contexto corporativo;\n* **Controle de alucinações:** Tendência a se ater aos fatos presentes nos documentos recuperados.\n\nOutros modelos testados:\n\n**EMBEDDINGS**\n* **Alibaba-NLP/gte-Qwen2-1.5B-instruct:** Excelente modelo, porém muito lento e demanda muita memória.\n* **sentence-transformers/all-MiniLM-L6-v2:** Extremamente veloz, baixíssimo consumo de memória, porém limitado a 256 tokens e idioma Inglês.\n* **sentence-transformers/all-mpnet-base-v2:** Limitado a 384 tokens e idioma inglês.\n\nNa base de dados utilizada neste PoC, *chunking* em fatias menores que 1024 tokens prejudicou o desempenho da busca, uma vez que os tópicos contidos nos documentos são longos, não cabiam em fatiamentos menores que 512 tokens, resultando em perdas semânticas ou respostas incompletas. Por esta razão, modelos pequenos como os acima foram descartados. Eles podem, contudo, ser úteis em outras bases de dados com tópicos mais concisos. \n\n**GERAÇÃO**\n* **deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B**\n* **deepseek-ai/DeepSeek-R1-Distill-Qwen-7B**\n* **deepseek-ai/DeepSeek-R1-Distill-Llama-8B**\n\nOs modelos DeepSeek foram descartados por serem modelos com *Reasoning/CoT*, o que era desnecessário para esta aplicação. O modelo DeepSeek V3 (sem *Reasoning*) possui demanda de memória extremamente alta para fins deste PoC, e por isso, foi substituído pelo Llama.","metadata":{}},{"cell_type":"markdown","source":"# Carregamento dos modelos\n\nNa próxima célula, são definidos o tokenizador para o modelo de LLM pré-treinado (selecionado na célula anterior) usando a classe *AutoTokenizer* da biblioteca *transformers*, e o modelo de geração de texto, utilizando a classe AutoModelForCausalLM.\n\nPara reduzir o consumo de memória, é possível reduzir a precisão dos pesos do LLM definindo o parâmetro *\"load_in_8bit\"* como *True*. Isso resultará em respostas menos precisas, mas é possível mitigar a deficiência reduzindo a complexidade das perguntas. \n\nEm seguida, é instanciado o modelo do Hugging Face hub para geração de embeddings de texto utilizando a classe HuggingFaceEmbeddings da biblioteca *langchain_huggingface*. Como parâmetros para a geração de embeddings foram especificadas a utilização de GPU compatível com CUDA e a normalização das embeddings. A normalização é importante para mecanismos de busca baseados em similaridade de cosseno, como o implementado neste projeto usando FAISS, para que a amplitude do vetor não influencie no *score* de similaridade, mas sim, apenas o ângulo entre os vetores.","metadata":{}},{"cell_type":"code","source":"tokenizer = transformers.AutoTokenizer.from_pretrained(llm_model_id)\nllm_model = transformers.AutoModelForCausalLM.from_pretrained(llm_model_id, device_map=\"auto\", load_in_8bit=False)\n\nembedding_model = HuggingFaceEmbeddings(\n    model_name=embeddings_model_name,\n    model_kwargs={\"device\": \"cuda\", # Alternar para \"cpu\" quando desejar usar a CPU para embeddings.\n                 },\n    encode_kwargs={\"normalize_embeddings\": True, # \"batch_size\": 16  # Para reduzir o consumo de memória, usar tamanhos de lotes menores com modelos maiores.\n                  }\n)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-06-06T01:47:02.928972Z","iopub.execute_input":"2025-06-06T01:47:02.929180Z","iopub.status.idle":"2025-06-06T01:48:38.465215Z","shell.execute_reply.started":"2025-06-06T01:47:02.929162Z","shell.execute_reply":"2025-06-06T01:48:38.463916Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Criação de uma classe de documentos personalizada\n\nNesta etapa definimos uma nova classe de objeto nomeada \"Document\" que armazenará os pedaços de texto dos documentos e metadados que serão usados para indexação, pesquisa e recuperação mais a frente.","metadata":{}},{"cell_type":"code","source":"class Document:\n    \"\"\"Classe para armazenamento do texto dos documentos junto dos metadados.\"\"\"\n    def __init__(self, text: str, metadata: Dict[str, Any]):\n        self.text = text\n        self.metadata = metadata\n    \n    def __repr__(self):\n        return f\"Document(metadata={self.metadata})\"","metadata":{"execution":{"iopub.status.busy":"2025-06-06T01:48:38.466316Z","iopub.execute_input":"2025-06-06T01:48:38.467215Z","iopub.status.idle":"2025-06-06T01:48:38.472353Z","shell.execute_reply.started":"2025-06-06T01:48:38.467184Z","shell.execute_reply":"2025-06-06T01:48:38.471587Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Carregamento e extração de texto\n\nA função abaixo carrega documentos PDF da pasta informada na variável \"pdf_folder\", extrai o texto e armazena-os em um objeto da classe Document criada anteriormente. Também são armazenados os metadados que registram o nome do arquivo PDF de origem e o número de páginas extraídas.\n\nA extração é feita por páginas para limitar o consumo de memória. Caso ocorram erros durante o processo de extração, eles são registrados no Logger e impressos na tela.","metadata":{}},{"cell_type":"code","source":"def extract_text_from_pdfs(pdf_folder_path: str) -> List[Document]:\n    \"\"\"\n    Extract text from all PDF files in a folder, with error handling and progress tracking.\n    \n    Args:\n        pdf_folder_path: Path to folder containing PDF files\n        \n    Returns:\n        List of Document objects containing text and metadata\n    \"\"\"\n    documents = []\n    pdf_files = [f for f in os.listdir(pdf_folder_path) if f.endswith('.pdf')]\n    \n    if not pdf_files:\n        logger.warning(f\"No PDF files found in {pdf_folder_path}\")\n        return []\n    \n    logger.info(f\"Processing {len(pdf_files)} PDF files from {pdf_folder_path}\")\n    \n    for filename in tqdm(pdf_files, desc=\"Extracting text from PDFs\"):\n        pdf_path = os.path.join(pdf_folder_path, filename)\n        \n        try:\n            with pdfplumber.open(pdf_path) as pdf:\n                pdf_text = ''\n                for page_num, page in enumerate(pdf.pages):\n                    try:\n                        page_text = page.extract_text() or \"\"\n                        pdf_text += page_text\n                    except Exception as e:\n                        logger.warning(f\"Error extracting text from page {page_num} in {filename}: {e}\")\n                \n                if pdf_text.strip():  # Only add if we have text\n                    document = Document(\n                        text=pdf_text,\n                        metadata={\n                            \"source\": pdf_path,\n                            \"filename\": filename,\n                            \"page_count\": len(pdf.pages)\n                        }\n                    )\n                    documents.append(document)\n                else:\n                    logger.warning(f\"No text extracted from {filename}\")\n        \n        except Exception as e:\n            logger.error(f\"Error processing PDF {filename}: {e}\")\n    \n    logger.info(f\"Successfully extracted text from {len(documents)} of {len(pdf_files)} PDFs\")\n    return documents","metadata":{"execution":{"iopub.status.busy":"2025-06-06T01:48:38.474295Z","iopub.execute_input":"2025-06-06T01:48:38.474493Z","iopub.status.idle":"2025-06-06T01:48:47.310954Z","shell.execute_reply.started":"2025-06-06T01:48:38.474476Z","shell.execute_reply":"2025-06-06T01:48:47.309882Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Chunking\n\nEsta função fatia o documento criado na extração em trechos de 1024 tokens. Por padrão, uma sobreposição de 50 tokens é mantida entre os trechos para preservar o contexto de cada divisão, mas o tamanho dessa sobreposição pode ser facilmente alterada por meio do parâmetro *\"overlap_size\"*.\n\nA função também atualiza os metadados incluindo um identificador para cada trecho, tamanho, número da palavra inicial e final no documento original para que seja possível recuperar o trecho extraído ou realçá-lo no documento original (não implementado).","metadata":{}},{"cell_type":"code","source":"def chunk_text(document: Document, chunk_size: int = 1024, overlap_size: int = 50) -> List[Document]:\n    \"\"\"\n    Split document text into chunks based on word count while preserving context with overlap.\n    \n    Args:\n        document: Document object containing text and metadata\n        chunk_size: Approximate number of words per chunk\n        overlap_size: Number of words to overlap between chunks\n        \n    Returns:\n        List of Document objects representing chunks\n    \"\"\"\n    text = document.text\n    words = text.split()\n    \n    if len(words) <= chunk_size:\n        # Document is small enough to be a single chunk\n        return [document]\n    \n    chunks = []\n    for i in range(0, len(words), chunk_size - overlap_size):\n        # Get the words for this chunk\n        chunk_words = words[i:i + chunk_size]\n        \n        if len(chunk_words) < 10:  # Skip very small trailing chunks\n            continue\n            \n        chunk_text = \" \".join(chunk_words)\n        \n        # Create a new Document with updated metadata\n        chunk_doc = Document(\n            text=chunk_text,\n            metadata={\n                **document.metadata,\n                \"chunk_id\": len(chunks),\n                \"chunk_start_word\": i,\n                \"chunk_end_word\": i + len(chunk_words),\n                \"chunk_size\": len(chunk_words),\n                \"is_chunk\": True\n            }\n        )\n        \n        chunks.append(chunk_doc)\n    \n    return chunks","metadata":{"execution":{"iopub.status.busy":"2025-06-06T01:48:47.312385Z","iopub.execute_input":"2025-06-06T01:48:47.312730Z","iopub.status.idle":"2025-06-06T01:48:48.297782Z","shell.execute_reply.started":"2025-06-06T01:48:47.312699Z","shell.execute_reply":"2025-06-06T01:48:48.296501Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Pipeline para processar PDFs e criar uma base vetorial pesquisável\n\nA função a seguir executa todo o pipeline para transformar uma coleção de PDFs em uma base vetorial pesquisável. Nela, os seguintes passos são realizados:\n\n**Extração de Texto:** Extrai o texto de todos os arquivos PDF em uma pasta especificada.<br>\n**Divisão em Chunks:** Divide o texto extraído em trechos menores (por padrão, 1024 tokens) com uma sobreposição entre trechos (por padrão, 50 tokens).<br>\n**Geração de Embeddings:** Cria embeddings para cada fatia de texto utilizando o modelo de embeddings pré-treinado especificado anteriormente.<br>\n**Criação e Indexação FAISS:** Cria um índice FAISS e adiciona os embeddings a este índice para permitir buscas por similaridade.<br>\n**Persistência:** Salva o índice FAISS e os metadados dos documentos em arquivos para uso posterior.<br>\n\nEm resumo, a função extrai o texto de todos os arquivos PDFs na pasta especificada, processa-os, cria um índice FAISS pesquisável e salva os resultados em disco.\n\nAqui, Logger é usado novamente para mostrar o progresso e eventuais erros no processamento dos dados.","metadata":{}},{"cell_type":"code","source":"def process_pdfs_and_create_vectorstore(pdf_folder_path: str,\n                                        faiss_index_filename: str = \"faiss_index.faiss\",\n                                        metadata_filename: str = \"faiss_metadata.json\",\n                                        chunk_size: int = 1024,\n                                        overlap_size: int = 50):\n    \"\"\"\n    Complete pipeline to process PDFs and create a searchable vector store with metadata.\n\n    Args:\n        pdf_folder_path: Path to folder containing PDF files\n        faiss_index_filename: Filename to save the FAISS index\n        metadata_filename: Filename to save the metadata\n        chunk_size: Number of tokens per chunk\n        overlap_size: Number of tokens to overlap between chunks\n\n    Returns:\n        Tuple of (FAISS index, list of document chunks, list of metadata)\n    \"\"\"\n    # Step 1: Extract text from PDFs\n    documents = extract_text_from_pdfs(pdf_folder_path)\n    if not documents:\n        logger.error(\"No documents were successfully processed\")\n        return None, [], []\n\n    # Step 2: Chunk text into smaller sections\n    all_chunks = []\n    for doc in documents:\n        chunks = chunk_text(doc, chunk_size, overlap_size)\n        all_chunks.extend(chunks)\n\n    logger.info(f\"Created {len(all_chunks)} chunks from {len(documents)} documents\")\n\n    # Step 3: Create embeddings for all chunks\n    texts = [chunk.text for chunk in all_chunks]\n    metadata = [\n        {**chunk.metadata, \"text\": chunk.text}\n        for chunk in all_chunks\n    ]\n\n    logger.info(\"Creating embeddings for all chunks\")\n    embeddings = []\n    for text in tqdm(texts, desc=\"Creating Embeddings\"):  # Wrap texts with tqdm to track progress.\n        embedding = embedding_model.embed_documents([text])[0]\n        embeddings.append(embedding)\n    embeddings = np.array(embeddings).astype('float32')\n\n    # Step 4: Create and save FAISS vector store\n    dim = embeddings.shape[1]\n    index = faiss.IndexFlatIP(dim)  # Inner product indexing with normalized vectors is similar to cosine similarity.\n    index.add(embeddings)\n\n    faiss.write_index(index, faiss_index_filename)\n    with open(metadata_filename, 'w') as f:\n        json.dump(metadata, f)\n\n    logger.info(f\"FAISS index with {index.ntotal} vectors saved to {faiss_index_filename}\")\n    logger.info(f\"Document metadata saved to {metadata_filename}\")\n\n    return index, all_chunks, metadata","metadata":{"execution":{"iopub.status.busy":"2025-06-06T01:48:48.299073Z","iopub.execute_input":"2025-06-06T01:48:48.299486Z","iopub.status.idle":"2025-06-06T01:48:49.545906Z","shell.execute_reply.started":"2025-06-06T01:48:48.299420Z","shell.execute_reply":"2025-06-06T01:48:49.544683Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Busca por Similaridade\n\nCom todo o pipeline de ETL pronto, ainda é preciso um mecanismo para recuperação dos documentos. A próxima função realiza a busca de documentos similares a uma dada consulta usando o índice FAISS criado na etapa anterior.\n\nA função:\n* Carrega o índice FAISS e os metadados dos documentos.<br>\n* Gera os embeddings da consulta utilizando o mesmo modelo de embeddings usado para indexar os documentos.<br>\n* Busca no índice FAISS os vetores mais próximos (mais similares) ao embedding da consulta.<br>\n* Retorna uma lista dos *top_k* resultados, contendo a pontuação de similaridade, os metadados e o texto de cada documento encontrado.\n\nVale notar que o tipo de indexação do FAISS (*IndexFlatIP* -- produto interno) com vetores normalizados é similar ao cálculo de similaridade de cosseno.","metadata":{}},{"cell_type":"code","source":"def search_similar_documents(query: str, index_path: str, metadata_path: str, top_k: int = 5):\n    \"\"\"\n    Search for documents similar to the query.\n    \n    Args:\n        query: Query text to search for\n        index_path: Path to the FAISS index file\n        metadata_path: Path to the metadata JSON file\n        top_k: Number of top results to return\n        \n    Returns:\n        List of tuples (similarity score, document metadata, document text)\n    \"\"\"\n    # Load the index\n    index = faiss.read_index(index_path)\n    \n    # Load the metadata\n    with open(metadata_path, 'r') as f:\n        metadata = json.load(f)\n    \n    # Encode the query\n    query_embedding = embedding_model.embed_documents([query])\n    query_embedding = normalize(np.array(query_embedding), axis=1).astype('float32')\n    \n    # Search the index\n    scores, indices = index.search(query_embedding, top_k)\n    \n    # Combine results\n    results = []\n    for score, idx in zip(scores[0], indices[0]):\n        if idx >= 0 and idx < len(metadata):  # Valid index\n            doc_metadata = metadata[idx]\n            doc_text = doc_metadata.pop(\"text\", \"Text not available\")\n            results.append((float(score), doc_metadata, doc_text))\n    \n    return results","metadata":{"execution":{"iopub.status.busy":"2025-06-06T01:48:49.547065Z","iopub.execute_input":"2025-06-06T01:48:49.547495Z","iopub.status.idle":"2025-06-06T01:48:50.600569Z","shell.execute_reply.started":"2025-06-06T01:48:49.547446Z","shell.execute_reply":"2025-06-06T01:48:50.599410Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Re-Ranking\n\nDurante testes, foi verificado que os resultados frequentemente ignoravam versóes de software quando eram mencionados. Por exemplo, em uma consulta sobre a compatibilidade e requisitos para uso de um determinado recurso no sistema operacional \"Windows 10\", os resultados de pesquisa ignoravam a versão 10 e traziam resultados genéricos para qualquer edição do Windows que encontrasse nos documentos.\n\nDentre os motivos, estão:\n\n* **Relevância dos Tokens:** Números e tokens curtos geralmente recebem menos peso nos modelos de embedding, pois são muito comuns em textos.\n\n* **Diluição de Contexto:** Quando as palavras-chaves de consulta aparecem com muita frequência nos documentos, ela domina o sinal semântico do documento, obscurecendo distinções sutis como \"Windows 10\".\n\n* **Viés de Pré-Treinamento:** Modelos de embedding podem não ter sido treinados para distinguir versões de produtos ou especificações técnicas como distinções semânticas relevantes.\n\nComo o modelo selecionado é consideravelmente pequeno em número de parâmetros, já era esperado que tivesse dificuldades com nuances desse tipo.\n\nDevido a isso, uma função de busca adicional foi criada para fazer *re-ranking* dos documentos pesquisados, por meio de impulsionamento (*boosting*) dos resultados que contêm a versão explícita do software na *query*, quando existir.\n\nA função de busca híbrida *technical_version_search* é uma implementação de busca por similaridade semântica com filtragem por expressões regulares e técnicas de *boosting* e *re-ranking* dos resultados. Quando o algoritmo detecta determinados padrões de versionamento de software na consulta *(ex: \"Windows 10\", \"iOS 18.2\")*, recupera mais documentos iniciais do que o especificado em *top_k* e dá pesos maiores para resultados que contêm correspondências exatas de versão, recalculando as pontuações para cada documento recuperado por similaridade semântica.\n\nQuando o padrão não é detectado na consulta, o algoritmo reverte para a função de busca padrão acima.\n\nA eficiência na recuperação de artigos técnicos teve uma melhora significativa quando este mecanismo de busca híbrido foi implementado. No entanto, modelos de embeddings maiores (ex.: 70B) podem não precisar deste recurso pois tendem a capturar essas nuances semânticas com maior assertividade.","metadata":{}},{"cell_type":"code","source":"def technical_version_search(query, index_path, metadata_path, top_k=5):\n    \"\"\"\n    Search optimized for technical product version queries\n    \n    Specifically addresses the problem of product version numbers\n    being overlooked in embedding-based search.\n    \"\"\"\n    \n    # 1. Detect version numbers in query\n    version_patterns = re.findall(r'(\\w+)\\s+(\\d+(?:\\.\\d+)*)', query)\n    \n    # If no product version is detected in the query, fall back to standard search\n    if not version_patterns:\n        return search_similar_documents(query, index_path, metadata_path, top_k)\n    \n    # 2. Load resources (FAISS index and JSON metadata)\n    index = faiss.read_index(index_path)\n    with open(metadata_path, 'r') as f:\n        metadata = json.load(f)\n    \n    # 3. Create semantic query embedding\n    query_embedding = embedding_model.embed_documents([query])\n    query_embedding = normalize(np.array(query_embedding), axis=1).astype('float32')\n    \n    # 4. Get more initial candidates than needed\n    initial_k = min(100, len(metadata))\n    scores, indices = index.search(query_embedding, initial_k)\n    \n    # 5. Get candidate documents\n    candidates = []\n    for i, idx in enumerate(indices[0]):\n        if idx >= 0 and idx < len(metadata):\n            text = metadata[idx].pop(\"text\", \"\")\n            candidates.append((float(scores[0][i]), metadata[idx], text))\n    \n    # 6. Apply version-specific filtering\n    filtered_results = []\n    \n    for score, meta, text in candidates:\n        version_match_score = 0\n        text_lower = text.lower()\n        \n        # Check for exact version matches\n        for product, version in version_patterns:\n            product_lower = product.lower()\n            version_pattern = fr\"{product_lower}\\s+{version}\\b\"\n            \n            # If exact version match found, give big boost\n            if re.search(version_pattern, text_lower):\n                version_match_score = 10.0  # Strong boost for exact version match\n            # Otherwise check for product name\n            elif product_lower in text_lower:\n                version_match_score = 0.5  # Small boost for just product\n        \n        # Combined score with heavy emphasis on version matching\n        combined_score = (score * 0.2) + (version_match_score * 0.8)\n        \n        # Only include results with some version relevance\n        if version_match_score > 0:\n            filtered_results.append((combined_score, meta, text))\n    \n    # If filtering gave us results, use them\n    if filtered_results:\n        filtered_results.sort(reverse=True, key=lambda x: x[0])\n        return filtered_results[:top_k]\n    \n    # Fall back to regular results\n    return candidates[:top_k]","metadata":{"execution":{"iopub.status.busy":"2025-06-06T01:48:50.601557Z","iopub.execute_input":"2025-06-06T01:48:50.601898Z","iopub.status.idle":"2025-06-06T01:48:50.678196Z","shell.execute_reply.started":"2025-06-06T01:48:50.601868Z","shell.execute_reply":"2025-06-06T01:48:50.677359Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Processamento dos PDFs\n\nO trecho abaixo executa o processamento completo dos PDFs para criar o índice vetorial.\n\n* Extrai texto dos PDFs na pasta especificada pela variável *pdf_folder*.\n* Divide os documentos em pedaços menores.\n* Gera embeddings para cada trecho de texto.\n* Cria e salva o índice FAISS junto dos metadados.","metadata":{}},{"cell_type":"code","source":"# Process PDFs and create vector store\nindex, chunks, metadata = process_pdfs_and_create_vectorstore(\n    pdf_folder,\n    faiss_index_filename=index_file,\n    metadata_filename=metadata_file\n)","metadata":{"execution":{"iopub.status.busy":"2025-06-06T01:48:50.678937Z","iopub.execute_input":"2025-06-06T01:48:50.679173Z","iopub.status.idle":"2025-06-06T01:50:36.802488Z","shell.execute_reply.started":"2025-06-06T01:48:50.679154Z","shell.execute_reply":"2025-06-06T01:50:36.801785Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Criação de pipeline de geração de texto\n\nNesta etapa, o pipeline de geração de texto é inicializado com os seguintes parâmetros:\n- max_new_tokens: limite de tokens a gerar (1024)\n- temperature: controle de aleatoriedade (0.5)\n- do_sample: habilita amostragem probabilística (True)\n- truncation: habilita truncamento de textos longos\n\nO número máximo de tokens (1024) junto da habilitação de truncamento foram escolhidos para limitar o consumo de memória, lentidão e para evitar erros ou resultados imprevisíveis. Os modelos escolhidos suportam essa janela escolhida. Como mencionado anteriormente, para os documentos da base de dados teste, janelas menores tiveram resultados com menor qualidade pois muitas das respostas envolvem contextos maiores que 512 tokens.\n\nO parâmetro *temperature* foi ajustado para 0.5 junto com *do_sample* = ***True***, pois neste caso, a intenção é permitir que o modelo retorne respostas moderadamente mais naturais, ao invés de meras cópias dos documentos de origem. Até 0.4, o modelo retornou respostas muito extensas ou prolixas. Já em temperaturas maiores que 0.5, as respostas ignoraram nuances e passos importantes das instruções.","metadata":{}},{"cell_type":"code","source":"pipe = transformers.pipeline(\n    \"text-generation\",\n    model=llm_model,\n    tokenizer=tokenizer,\n    device_map=\"auto\",\n    max_new_tokens=1024,\n    temperature=0.5,\n    do_sample=True,\n    truncation=True\n)\n\nllm = HuggingFacePipeline(pipeline=pipe)","metadata":{"execution":{"iopub.status.busy":"2025-06-06T02:52:13.379956Z","iopub.execute_input":"2025-06-06T02:52:13.380252Z","iopub.status.idle":"2025-06-06T02:52:13.386218Z","shell.execute_reply.started":"2025-06-06T02:52:13.380231Z","shell.execute_reply":"2025-06-06T02:52:13.385558Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# RAG (Retrieval-Augmented Generation)\n\nA função *answer_query* implementa o componente principal do sistema RAG: prover respostas às consultas do usuário.\n\nParâmetros:\n- query: string de consulta do usuário\n- index_path: caminho para o índice FAISS\n- metadata_path: caminho para o arquivo de metadados\n- top_k: número de documentos a recuperar\n- debug: se *True*, exibe informações adicionais de depuração (id do documento, nome do arquivo de origem, id do chunk, pontuação no ranking de similaridade, o contexto recuperado, além do Chain-of-Thought, caso modelos com Reasoning sejam utilizados. Por padrão, esta opção é definida como *False* para obtermos uma resposta mais limpa.\n\nAqui a variável top_k é limitada a 2 documentos para evitar estourar o limite de contexto do modelo de LLM escolhido (o Llama 3.2 3B-Instruct é capaz de processar até 4096 tokens). Modelos superiores têm demanda de memória que excedem as metas deste projeto e utilizar quantização neles poderia resultar em desempenho inferior.\n\nEsta função também passa instruções ao LLM sobre como responder às consultas, atendo-se estritamente ao contexto. Caso a informação solicitada não esteja no contexto, a LLM é instruída a responder que não possui informações suficientes para responder a pergunta.","metadata":{}},{"cell_type":"code","source":"def answer_query(query: str, index_path: str, metadata_path: str, top_k: int = 4, debug: bool = True):\n    \"\"\"\n    Answer a query using retrieved documents and LLM.\n    \n    Args:\n        query: User query string\n        index_path: Path to FAISS index\n        metadata_path: Path to metadata file\n        top_k: Number of documents to retrieve\n        debug: If True, prints additional debugging information\n        \n    Returns:\n        LLM answer based on retrieved context.\n    \"\"\"\n    # Retrieve relevant documents\n    search_results = technical_version_search(query, index_path, metadata_path, top_k)\n    \n    # Show debug info if requested\n    if debug:\n        print(f\"Retrieved {len(search_results)} documents:\")\n        for i, (score, metadata, _) in enumerate(search_results):\n            filename = metadata.get(\"filename\", \"Unknown\")\n            chunk_id = metadata.get(\"chunk_id\", \"N/A\")\n            print(f\"  Doc {i+1}: Score={score:.2f}, File={filename}, Chunk={chunk_id}\")\n    \n    # Build context from retrieved documents for the LLM\n    context = \"\"\n    for _, _, text in search_results:\n        context += f\"\\n{text}\\n\"\n    \n    # Create prompt with context and query\n    prompt = f\"\"\"You are an AI assistant tasked with answering questions based on the provided documents.\nPlease answer the question based only on the context provided. If the context doesn't contain the \ninformation needed to answer the question, say \"I don't have enough information to answer this question.\"\n\nContext:\n{context}\n\nQuestion: {query}\n\nAnswer:\"\"\"\n    \n    # Get full response from LLM\n    full_response = llm(prompt)\n    \n    # Extract just the answer part from the response\n    # Most LLMs will repeat \"Answer:\" before giving their answer\n    if \"Answer:\" in full_response:\n        # Split by \"Answer:\" and take everything after it\n        answer = full_response.split(\"Answer:\", 1)[1].strip()\n    else:\n        # If no \"Answer:\" marker is found, try to extract just the generated part\n        # This is trickier and less reliable, but we'll do our best\n        # Find where our prompt ends and extract everything after that\n        if prompt in full_response:\n            answer = full_response[len(prompt):].strip()\n        else:\n            # Last resort: just return the full response\n            answer = full_response\n    \n    return answer","metadata":{"execution":{"iopub.status.busy":"2025-06-06T02:49:54.551930Z","iopub.execute_input":"2025-06-06T02:49:54.552253Z","iopub.status.idle":"2025-06-06T02:49:54.558763Z","shell.execute_reply.started":"2025-06-06T02:49:54.552226Z","shell.execute_reply":"2025-06-06T02:49:54.557909Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Interface interativa para o sistema RAG\n\nPor fim, a última função cria uma interface interativa para a sessào de perguntas e respostas. Para encerrar o loop, basta o usuário digitar 'sair'.","metadata":{}},{"cell_type":"code","source":"def run_rag_query(faiss_index_filename: str = \"faiss_index.faiss\",\n                  metadata_filename: str = \"faiss_metadata.json\"):\n    \n    \"\"\"Interface interativa para consulta RAG.\"\"\"\n    \n    print(\"Seja bem-vindo ao Sistema de Consultas de TI!\\n\")\n        \n    while True:\n        query = input(\"\\nO que deseja saber? (ou digite 'sair' para encerrar): \")\n        if query.lower() == 'sair':\n            break\n            \n        print(\"\\nBuscando informação...\")\n        answer = answer_query(query, index_file, metadata_file)\n        print(\"\\nResposta:\")\n        print(answer)\n\n# Run the interactive interface\nif __name__ == \"__main__\":\n    run_rag_query(index_file, metadata_file)","metadata":{"execution":{"iopub.status.busy":"2025-06-06T02:14:58.612102Z","iopub.execute_input":"2025-06-06T02:14:58.612394Z","iopub.status.idle":"2025-06-06T02:44:22.530230Z","shell.execute_reply.started":"2025-06-06T02:14:58.612372Z","shell.execute_reply":"2025-06-06T02:44:22.529505Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Análise das Respostas\n\nPara a avaliação do algoritmo, foram preparadas perguntas com diferentes níveis de complexidade sobre o conteúdo dos documentos. Algumas são perguntas cujas respostas são endereçadas diretamente no documento. Outras são mais complexas porque as respostas podem estar em tabelas ou ocupar múltiplos trechos do documento.\n\n1. Can you tell me the difference between Autopilot and Autopilot device preparation?\n2. What are the requirements for Windows Autopilot device preparation?\n3. How corporate identifiers are set?\n4. Is Windows 10 devices compatible with Windows Autopilot Device Preparation?\n5. What is the process for a successful registration of a device?\n\nAs perguntas foram feitas com três níveis de temperatura diferentes: 0.5, 0.2, 0.1. As respostas para cada caso e as observações estão no arquivo [CompareResults.xlsx](https://github.com/fabiofaria-git/BIMaster-Proj/raw/refs/heads/main/CompareResults.xlsx) no repositório.\n\n- A variação de temperatura não afetou consideravelmente o resultado com os modelos escolhidos. \n\n- O modelo respondeu corretamente às perguntas 2, 3, 4 e 5 em todos os níveis de temperatura testados.\n\n- As respostas para a pergunta 1 foram parcialmente corretas e incompletas.\n\nA baixa qualidade nas respostas da pergunta 1 evidencia uma deficiência da solução proposta neste PoC: a resposta para a pergunta está em uma longa tabela.\n\n![Tabela com as diferenças solicitadas na pergunta 1](https://raw.githubusercontent.com/fabiofaria-git/BIMaster-Proj/refs/heads/main/AutopilotvsDevicePrep.png)\n\nPDFs não armazenam tabelas como estruturas semânticas, apenas como posicionamento de texto. Há uma perda de estrutura relacional em tabelas se elas não forem tratadas separadamente quando detectadas. Esta deficiência já era prevista. Porém, como os documentos de instruções a equipes de suporte de T.I. raramente contêm tabelas e a implementação de uma solução aumentaria substancialmente a complexidade de todo o pipeline, optou-se por não implementar tal funcionalidade.","metadata":{}},{"cell_type":"markdown","source":"# Conclusões\n\nEste PoC demonstra que Large Language Models relativamente pequenos e gratuitos podem viabilizar financeiramente projetos de Inteligência Artificial em ambientes corporativos que não permitem uso de serviços em núvem, mas ainda assim trazer resultados satisfatórios.\n\nCabe notar que há modelos mais complexos tanto de geração de texto quanto de embeddings que podem melhorar a versatilidade da solução e a qualidade das respostas. Entretanto, um dos maiores entraves no uso desses modelos maiores é o consumo de memória e o tempo de processamento das respostas. Portanto, a relação custo/benefício precisa ser avaliada caso a caso de acordo com os recursos disponíveis.\n\nDito isto, este projeto permite algumas melhorias com aumento modesto de demanda de memória e de processamento:\n\n1) Implementação de pipeline multimodal que reconheça tabelas, imagens, legendas e URLs;\n2) Separação do pipeline ETL do pipeline RAG;\n3) *Semantinc chunking* e *Contextual Chunk Headers (CCH)*.\n\nA primeira permitiria extração e armazenamento de imagens, tabelas, legendas e URLs em blocos, como metadados de cada trecho do documento original, garantindo a preservação das relações semânticas nas buscas e consultas. Isso também permitiria que as respostas incluíssem as tabelas e imagens dos documentos de origem.\n    \nA segunda melhoria proposta resolveria outra deficiência da versão atual desta aplicação: como a cada execução do pipeline, a base vetorial é recriada, há uma variância no processo de chunking e indexação que faz com que a consistência das respostas do modelo variem ligeiramente entre uma execução e outra. Para um ambiente de produção, o recomendável seria separar esses dois pipelines para executar o processo de extração, chunking e indexação em base vetorial somente quando houverem alterações nos documentos da base de dados ou em intervalos regulares (por exemplo, quizenalmente, semanalmente ou mensalmente), garantindo maior consistência nas consultas.\n\nJá a terceira sugestão melhoraria principalmente a recuperação dos documentos mais relevantes. A técnica de *Contextual Chunk Headers (CCH)* usa o modelo de LLM para criar resumos de cada trecho do documento original e armazená-los como metadados antes da criação de embeddings, melhorando a eficácia na recuperação dos dados. Já a técnica de *Semantic Chunking* consiste em fracionar o texto em trechos que mantenham a coesão do contexto, mantendo secções que falam de um mesmo assunto no mesmo *chunk*. *CCH* e *Semântic Chunking* são técnicas bastante custosas computacionalmente e, por esta razão, foram deixadas de fora deste PoC, mas podem ser implementadas em conjunto com a separação do pipeline de ETL e executadas apenas ocasionalmente, minimizando o impacto financeiro desta solução, caso o custo/benefício justificar.","metadata":{}}]}